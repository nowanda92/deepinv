import warnings
import torchvision.utils
from deepinv.utils import (
    save_model,
    AverageMeter,
    get_timestamp,
    cal_psnr,
)
from deepinv.utils import plot, plot_curves, wandb_plot_curves, rescale_img, zeros_like
from deepinv.physics import Physics
import numpy as np
from tqdm import tqdm
import torch
import wandb
from pathlib import Path
from typing import Union
from dataclasses import dataclass, field
from deepinv.utils import PSNR


@dataclass
class Trainer:
    r"""
    Trainer class for training a reconstruction network.

    This class trains a reconstruction network using a set of losses, a forward operator, and a dataset.
    The class provides a flexible training loop that can be customized by the user. In particular, the user can
    rewrite the ``compute_loss``  method to define their custom training step without having to write all the training code
    from scratch:


    ::

        def compute_loss(self, physics, x, y, train=True):
            logs = {}

            self.optimizer.zero_grad()

            # Evaluate reconstruction network
            x_net = self.model_inference(y=y, physics=physics)

            # Compute the losses
            loss_total = 0
            for k, l in enumerate(self.losses):
                loss = l(x=x, x_net=x_net, y=y, physics=physics, model=self.model)
                loss_total += self.fact_losses[k] * loss
                if len(self.losses) > 1 and self.verbose_individual_losses:
                    current_log = (
                        self.logs_losses_train[k] if train else self.logs_losses_eval[k]
                    )
                    current_log.update(loss.item())
                    cur_loss = current_log.avg
                    logs[l.__class__.__name__] = cur_loss

            current_log = self.logs_total_loss_train if train else self.logs_total_loss_eval
            current_log.update(loss_total.item())
            logs[f"TotalLoss"] = current_log.avg

            if train:
                loss_total.backward()  # Backward the total loss

                norm = self.check_clip_grad()  # Optional gradient clipping
                if norm is not None:
                    logs["gradient_norm"] = self.check_grad_val.avg

                # Optimizer step
                self.optimizer.step()

            return x_net, logs


    If the user wants to change the way the metrics are computed, they can rewrite the ``compute_metrics`` method.
    The user can also change the way the samples are generated by rewriting the ``get_samples_online`` or ``get_samples_offline`` methods.

    .. note::

        The losses can be chosen from :ref:`the libraries' training losses <loss>`, or can be a custom loss function,
        as long as it takes as input ``(x, x_net, y, physics, model)`` and returns a scalar, where ``x`` is the ground
        reconstruction, ``x_net`` is the network reconstruction :math:`\inversef{y}{A}`,
        ``y`` is the measurement vector, ``physics`` is the forward operator
        and ``model`` is the reconstruction network. Note that not all inpus need to be used by the loss,
        e.g., self-supervised losses will not make use of ``x``.

    :param torch.utils.data.DataLoader train_dataloader: Train dataloader.
    :param int epochs: Number of training epochs.
    :param torch.nn.Module, list of torch.nn.Module losses: Loss or list of losses used for training the model.
    :param torch.utils.data.DataLoader eval_dataloader: Evaluation dataloader.
    :param deepinv.physics.Physics, list[deepinv.physics.Physics] physics: Forward operator(s)
        used by the reconstruction network at train time.
    :param torch.nn.optim optimizer: Torch optimizer for training the network.
    :param float grad_clip: Gradient clipping value for the optimizer. If None, no gradient clipping is performed.
    :param torch.nn.optim scheduler: Torch scheduler for changing the learning rate across iterations.
    :param torch.device device: gpu or cpu.
    :param int ckp_interval: The model is saved every ``ckp_interval`` epochs.
    :param int eval_interval: Number of epochs between each evaluation of the model on the evaluation set.
    :param str save_path: Directory in which to save the trained model.
    :param bool verbose: Output training progress information in the console.
    :param bool unsupervised: Train an unsupervised network, i.e., uses only measurement vectors y for training.
    :param bool plot_images: Plots reconstructions every ``ckp_interval`` epochs.
    :param bool wandb_vis: Use Weights & Biases visualization, see https://wandb.ai/ for more details.
    :param dict wandb_setup: Dictionary with the setup for wandb, see https://docs.wandb.ai/quickstart for more details.
    :param bool online_measurements: Generate the measurements in an online manner at each iteration by calling
        ``physics(x)``. This results in a wider range of measurements if the physics' parameters, such as
        parameters of the forward operator or noise realizations, can change between each sample; these are updated
        with the ``physics.reset()`` method. If ``online_measurements=False``, the measurements are loaded from the training dataset
    :param bool plot_measurements: Plot the measurements y. default=True.
    :param bool check_grad: Check the gradient norm at each iteration.
    :param str ckpt_pretrained: path of the pretrained checkpoint. If None, no pretrained checkpoint is loaded.
    :param list fact_losses: List of factors to multiply the losses. If None, all losses are multiplied by 1.
    :param int freq_plot: Frequency of plotting images to wandb during train evaluation (at the end of each epoch). If 1, plots at each epoch.
    :param bool verbose_individual_losses: If ``True``, the value of individual losses are printed during training. Otherwise, only the total loss is printed.
    """

    epochs: int
    losses: list
    physics: Physics = None
    optimizer: torch.optim.Optimizer = None
    grad_clip: float = None
    scheduler: torch.optim.lr_scheduler.LRScheduler = None
    metrics: Union[torch.nn.Module, list[torch.nn.Module]] = PSNR()
    device: Union[str, torch.device] = "cpu"
    ckp_interval: int = 1
    eval_interval: int = 1
    save_path: Union[str, Path] = "."
    verbose: bool = False
    unsupervised: bool = False
    plot_images: bool = False
    plot_metrics: bool = False
    wandb_vis: bool = False
    wandb_setup: dict = field(default_factory=dict)
    online_measurements: bool = False
    plot_measurements: bool = True
    check_grad: bool = False
    ckpt_pretrained: Union[str, None] = None
    fact_losses: list = None
    freq_plot: int = 1
    verbose_individual_losses: bool = True
    display_losses_eval: bool = False

    def setup_train(self, eval=False):
        r"""
        Set up the training process.

        It initializes the wandb logging, the different metrics, the save path, the physics and dataloaders,
        and the pretrained checkpoint if given.
        """

        self.save_path = Path(self.save_path)

        if self.wandb_setup is not None and not self.wandb_vis:
            warnings.warn(
                "wandb_vis is False but wandb_setup is provided. Wandb visualization deactivated (wandb_vis=False)."
            )

        # wandb initialiation
        if self.wandb_vis:
            if wandb.run is None:
                wandb.init(**self.wandb_setup)

        if not isinstance(self.losses, list) or isinstance(self.losses, tuple):
            self.losses = [self.losses]
        if not isinstance(self.metrics, list) or isinstance(self.metrics, tuple):
            self.metrics = [self.metrics]
        if self.fact_losses is None:
            self.fact_losses = [1] * len(self.losses)

        # losses
        self.logs_total_loss_train = AverageMeter("Training loss", ":.2e")
        self.logs_losses_train = [
            AverageMeter("Training loss " + l.name, ":.2e") for l in self.losses
        ]
        if eval:
            self.logs_total_loss_eval = AverageMeter("Validation loss", ":.2e")
            self.logs_losses_eval = [
                AverageMeter("Validation loss " + l.name, ":.2e") for l in self.losses
            ]

        # metrics
        self.logs_metrics_train = [
            AverageMeter("Training metric " + l.__class__.__name__, ":.2e")
            for l in self.metrics
        ]
        if eval:
            self.logs_metrics_eval = [
                AverageMeter("Validation metric " + l.__class__.__name__, ":.2e")
                for l in self.metrics
            ]

        # gradient clipping
        if self.check_grad:
            self.check_grad_val = AverageMeter("Gradient norm", ":.2e")

        self.save_path = f"{self.save_path}/{get_timestamp()}"

        # count the overall training parameters
        params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"The model has {params} trainable parameters")

        # make physics and data_loaders of list type
        if type(self.physics) is not list:
            self.physics = [self.physics]

        self.loss_history = []

        self.epoch_start = 0
        if self.ckpt_pretrained is not None:
            checkpoint = torch.load(self.ckpt_pretrained)
            self.model.load_state_dict(checkpoint["state_dict"])
            self.optimizer.load_state_dict(checkpoint["optimizer"])
            self.epoch_start = checkpoint["epoch"]

    def prepare_images(self, physics_cur, x, y, x_net):
        with torch.no_grad():
            if (
                self.plot_measurements
                and len(y.shape) == len(x.shape)
                and y.shape != x.shape
            ):
                y_reshaped = torch.nn.functional.interpolate(y, size=x.shape[2])
                if hasattr(physics_cur, "A_adjoint"):
                    imgs = [y_reshaped, physics_cur.A_adjoint(y), x_net, x]
                    caption = (
                        "From top to bottom: input, backprojection, output, target"
                    )
                    titles = ["Input", "Backprojection", "Output", "Target"]
                else:
                    imgs = [y_reshaped, x_net, x]
                    titles = ["Input", "Output", "Target"]
                    caption = "From top to bottom: input, output, target"
            else:
                if hasattr(physics_cur, "A_adjoint"):
                    if isinstance(physics_cur, torch.nn.DataParallel):
                        back = physics_cur.module.A_adjoint(y)
                    else:
                        back = physics_cur.A_adjoint(y)
                    imgs = [back, x_net, x]
                    titles = ["Backprojection", "Output", "Target"]
                    caption = "From top to bottom: backprojection, output, target"
                else:
                    imgs = [x_net, x]
                    caption = "From top to bottom: output, target"
                    titles = ["Output", "Target"]

            vis_array = torch.cat(imgs, dim=0)
            for i in range(len(vis_array)):
                vis_array[i] = rescale_img(vis_array[i], rescale_mode="min_max")
            grid_image = torchvision.utils.make_grid(vis_array, nrow=y.shape[0])

        return imgs, titles, grid_image, caption

    def log_metrics_wandb(self, log_dict_iter):
        r"""
        Log the metrics to wandb.
        """
        if self.wandb_vis:
            wandb.log(log_dict_iter)

    def check_clip_grad(self):
        r"""
        Check the gradient norm and perform gradient clipping if necessary.

        """
        out = None

        if self.grad_clip is not None:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)

        if self.check_grad:
            # from https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961/7
            grads = [
                param.grad.detach().flatten()
                for param in self.model.parameters()
                if param.grad is not None
            ]
            norm_grads = torch.cat(grads).norm()
            out = norm_grads.item()
            self.check_grad_val.update(norm_grads.item())

        return out

    def get_samples_online(self, iterators, g):
        r"""
        Get the samples for the online measurements.

        In this setting, a new sample is generated at each iteration by calling the physics operator.
        This function returns a dictionary containing necessary data for the model inference. It needs to contain
        the measurement, the ground truth, and the current physics operator, but can also contain additional data.

        :param list iterators: List of dataloader iterators.
        :param int g: Current dataloader index.
        :returns: a dictionary containing at least: the ground truth, the measurement, and the current physics operator.
        """
        x, _ = next(
            iterators[g]
        )  # In this case the dataloader outputs also a class label
        x = x.to(self.device)
        physics_cur = self.physics[g]

        y = physics_cur(x)

        return x, y, physics_cur

    def get_samples_offline(self, iterators, g):
        r"""
        Get the samples for the offline measurements.

        In this setting, samples have been generated offline and are loaded from the dataloader.
        This function returns a dictionary containing necessary data for the model inference. It needs to contain
        the measurement, the ground truth, and the current physics operator, but can also contain additional data.

        :param list iterators: List of dataloader iterators.
        :param int g: Current dataloader index.
        :returns: a dictionary containing at least: the ground truth, the measurement, and the current physics operator.
        """
        if self.unsupervised:
            y = next(iterators[g])
            x = None
        else:
            x, y = next(iterators[g])
            if type(x) is list or type(x) is tuple:
                x = [s.to(self.device) for s in x]
            else:
                x = x.to(self.device)

        y = y.to(self.device)
        physics_cur = self.physics[g]

        return x, y, physics_cur

    def get_samples(self, iterators, g):
        r"""
        Get the samples.

        This function returns a dictionary containing necessary data for the model inference. It needs to contain
        the measurement, the ground truth, and the current physics operator, but can also contain additional data.

        :param list iterators: List of dataloader iterators.
        :param int g: Current dataloader index.
        :returns: the tuple returned by the get_samples_online or get_samples_offline function.
        """
        if self.online_measurements:  # the measurements y are created on-the-fly
            samples = self.get_samples_online(iterators, g)
        else:  # the measurements y were pre-computed
            samples = self.get_samples_offline(iterators, g)

        return samples

    def model_inference(self, y, physics):
        r"""
        Perform the model inference.

        It returns the network reconstruction given the samples.

        :param torch.Tensor y: Measurement.
        :param deepinv.physics.Physics physics: Current physics operator.
        :returns: The network reconstruction.
        """
        y = y.to(self.device)
        x_net = self.model(y, physics)
        return x_net

    def compute_loss(self, physics, x, y, train=True):
        r"""
        Compute the loss and perform the backward pass.

        It evaluates the reconstruction network, computes the losses, and performs the backward pass.

        :param deepinv.physics.Physics physics: Current physics operator.
        :param torch.Tensor x: Ground truth.
        :param torch.Tensor y: Measurement.
        :param bool train: If ``True``, the model is trained, otherwise it is evaluated.
        :returns: (tuple) The network reconstruction x_net (for plotting and computing metrics) and
            the logs (for printing the training progress).
        """
        logs = {}

        self.optimizer.zero_grad()

        # Evaluate reconstruction network
        x_net = self.model_inference(y=y, physics=physics)

        if train or self.display_losses_eval:
            # Compute the losses
            loss_total = 0
            for k, l in enumerate(self.losses):
                loss = l(x=x, x_net=x_net, y=y, physics=physics, model=self.model)
                loss_total += self.fact_losses[k] * loss
                if len(self.losses) > 1 and self.verbose_individual_losses:
                    current_log = (
                        self.logs_losses_train[k] if train else self.logs_losses_eval[k]
                    )
                    current_log.update(loss.item())
                    cur_loss = current_log.avg
                    logs[l.__class__.__name__] = cur_loss

            current_log = (
                self.logs_total_loss_train if train else self.logs_total_loss_eval
            )
            current_log.update(loss_total.item())
            logs[f"TotalLoss"] = current_log.avg

        if train:
            loss_total.backward()  # Backward the total loss

            norm = self.check_clip_grad()  # Optional gradient clipping
            if norm is not None:
                logs["gradient_norm"] = self.check_grad_val.avg

            # Optimizer step
            self.optimizer.step()

        return x_net, logs

    def compute_metrics(self, x, x_net, y, physics, logs, train=True):
        r"""
        Compute the metrics.

        It computes the metrics over the batch.

        :param torch.Tensor x: Ground truth.
        :param torch.Tensor x_net: Network reconstruction.
        :param torch.Tensor y: Measurement.
        :param deepinv.physics.Physics physics: Current physics operator.
        :param dict logs: Dictionary containing the logs for printing the training progress.
        :param bool train: If ``True``, the model is trained, otherwise it is evaluated.
        :returns: The logs with the metrics.
        """
        # Compute the metrics over the batch
        with torch.no_grad():
            for k, l in enumerate(self.metrics):
                metric = l(x=x, x_net=x_net, y=y, physics=physics)

                current_log = (
                    self.logs_metrics_train[k] if train else self.logs_metrics_eval[k]
                )
                current_log.update(metric.cpu().numpy())
                logs[l.__class__.__name__] = current_log.avg

        return logs

    def step(self, epoch, progress_bar, train=True, last_batch=False):
        r"""
        Train/Eval a batch.

        It performs the forward pass, the backward pass, and the evaluation at each iteration.

        :param int epoch: Current epoch.
        :param tqdm progress_bar: Progress bar.
        :param bool train: If ``True``, the model is trained, otherwise it is evaluated.
        :param bool last_batch: If ``True``, the last batch of the epoch is being processed.
        :returns: The current physics operator, the ground truth, the measurement, and the network reconstruction.
        """

        # random permulation of the dataloaders
        G_perm = np.random.permutation(self.G)

        for g in G_perm:  # for each dataloader
            x, y, physics_cur = self.get_samples(self.current_iterators, g)

            # Compute loss and perform backprop
            x_net, logs = self.compute_loss(physics_cur, x, y, train=train)

            # Log metrics
            logs = self.compute_metrics(x, x_net, y, physics_cur, logs)

            # Update the progress bar
            progress_bar.set_postfix(logs)

        if last_batch:
            logs["step"] = epoch
            self.log_metrics_wandb(logs)  # Log metrics to wandb
            self.plot(epoch, physics_cur, x, y, x_net, train=train)  # Plot images

    def plot(self, epoch, physics, x, y, x_net, train=True):
        r"""
        Plot the images.

        It plots the images at the end of each epoch.

        :param int epoch: Current epoch.
        :param deepinv.physics.Physics physics: Current physics operator.
        :param torch.Tensor x: Ground truth.
        :param torch.Tensor y: Measurement.
        :param torch.Tensor x_net: Network reconstruction.
        :param bool train: If ``True``, the model is trained, otherwise it is evaluated.
        """
        post_str = "Training" if train else "Eval"
        if self.plot_images and ((epoch + 1) % self.freq_plot == 0):
            imgs, titles, grid_image, caption = self.prepare_images(
                physics, x, y, x_net
            )

            plot(
                imgs,
                titles=titles,
                show=self.plot_images,
                return_fig=True,
            )

            if self.wandb_vis:
                log_dict_post_epoch = {}
                images = wandb.Image(
                    grid_image,
                    caption=caption,
                )
                log_dict_post_epoch[post_str + " samples"] = images
                log_dict_post_epoch["step"] = epoch
                wandb.log(log_dict_post_epoch)

    def train(self, model, train_dataloader, eval_dataloader=None):
        r"""
        Train the model.

        It performs the training process, including the setup, the evaluation, the forward and backward passes,
        and the visualization.


        :param torch.nn.Module, deepinv.models.ArtifactRemoval model: Reconstruction network, which can be PnP, unrolled, artifact removal
            or any other custom reconstruction network.
        :returns: The trained model.
        """

        self.model = model

        if type(train_dataloader) is not list:
            train_dataloader = [train_dataloader]
        if eval_dataloader and type(eval_dataloader) is not list:
            eval_dataloader = [eval_dataloader]

        self.G = len(train_dataloader)

        self.setup_train(eval=eval_dataloader is not None)
        for epoch in range(self.epoch_start, self.epochs):

            ## Evaluation
            perform_eval = (
                (not self.unsupervised)
                and eval_dataloader
                and ((epoch + 1) % self.eval_interval == 0 or epoch + 1 == self.epochs)
            )
            if perform_eval:
                self.current_iterators = [iter(loader) for loader in eval_dataloader]
                batches = len(eval_dataloader[self.G - 1]) - int(
                    eval_dataloader[self.G - 1].drop_last
                )

                self.model.eval()
                for i in (
                    progress_bar := tqdm(
                        range(batches), ncols=150, disable=not self.verbose
                    )
                ):
                    progress_bar.set_description(f"Eval epoch {epoch + 1}")
                    self.step(
                        epoch, progress_bar, train=False, last_batch=(i == batches - 1)
                    )

                self.eval_psnr = self.logs_metrics_eval[0].avg

            ## Training
            self.current_iterators = [iter(loader) for loader in train_dataloader]
            batches = len(train_dataloader[self.G - 1]) - int(
                train_dataloader[self.G - 1].drop_last
            )

            self.model.train()
            for i in (
                progress_bar := tqdm(
                    range(batches), ncols=150, disable=not self.verbose
                )
            ):
                progress_bar.set_description(f"Train epoch {epoch + 1}")
                self.step(
                    epoch, progress_bar, train=True, last_batch=(i == batches - 1)
                )

            self.loss_history.append(self.logs_total_loss_train.avg)

            if self.scheduler:
                self.scheduler.step()

            # Saving the model
            save_model(
                epoch,
                self.model,
                self.optimizer,
                self.ckp_interval,
                self.epochs,
                self.loss_history,
                str(self.save_path),
                eval_psnr=self.eval_psnr if perform_eval else None,
            )

        if self.wandb_vis:
            wandb.save("model.h5")

        return self.model

    def test(self, model, test_dataloader):
        r"""
        Test the model.

        It computes the quality metrics of the reconstruction network on the test set.

        :param torch.nn.Module, deepinv.models.ArtifactRemoval model: Reconstruction network, which can be PnP,
            unrolled, artifact removal or any other custom reconstruction network.
        :param torch.utils.data.DataLoader test_dataloader: Test data loader, which should provide a tuple of (x, y) pairs.

        """

        self.model = model

        if type(test_dataloader) is not list:
            test_dataloader = [test_dataloader]

        self.current_iterators = [iter(loader) for loader in test_dataloader]

        batches = len(test_dataloader[self.G - 1]) - int(
            test_dataloader[self.G - 1].drop_last
        )

        self.model.eval()
        for _ in (
            progress_bar := tqdm(range(batches), ncols=150, disable=not self.verbose)
        ):
            progress_bar.set_description(f"Test")
            self.step(1, progress_bar, train=False, last_batch=True)

        self.eval_psnr = self.logs_metrics_eval[0].avg

        return self.logs_metrics_eval[0].avg


def test(
    model,
    test_dataloader,
    physics,
    device="cpu",
    plot_images=False,
    save_folder="results",
    plot_metrics=False,
    verbose=True,
    plot_only_first_batch=True,
    step=0,
    online_measurements=False,
    plot_measurements=True,
    img_interval=1,
    **kwargs,
):
    r"""
    Tests a reconstruction model (algorithm or network).

    This function computes the PSNR of the reconstruction network on the test set,
    and optionally plots the reconstructions as well as the metrics computed along the iterations.
    Note that by default only the first batch is plotted.

    :param torch.nn.Module, deepinv.models.ArtifactRemoval model: Reconstruction network, which can be PnP, unrolled, artifact removal
        or any other custom reconstruction network.
    :param torch.utils.data.DataLoader test_dataloader: Test data loader, which should provide a tuple of (x, y) pairs.
        See :ref:`datasets <datasets>` for more details.
    :param deepinv.physics.Physics, list[deepinv.physics.Physics] physics: Forward operator(s)
        used by the reconstruction network at test time.
    :param torch.device device: gpu or cpu.
    :param bool plot_images: Plot the ground-truth and estimated images.
    :param str save_folder: Directory in which to save plotted reconstructions.
    :param bool plot_metrics: plot the metrics to be plotted w.r.t iteration.
    :param bool verbose: Output training progress information in the console.
    :param bool plot_only_first_batch: Plot only the first batch of the test set.
    :param int step: Step number.
    :param bool online_measurements: Generate the measurements in an online manner at each iteration by calling
        ``physics(x)``.
    :param bool plot_measurements: Plot the measurements y. default=True.
    :param int img_interval: how many steps between plotting images
    :returns: A tuple of floats (test_psnr, test_std_psnr, linear_std_psnr, linear_std_psnr) with the PSNR of the
        reconstruction network and a simple linear inverse on the test set.
    """
    save_folder = Path(save_folder)

    psnr_init = []
    psnr_net = []

    model.eval()

    if type(physics) is not list:
        physics = [physics]

    if type(test_dataloader) is not list:
        test_dataloader = [test_dataloader]

    G = len(test_dataloader)

    show_operators = 5

    for g in range(G):
        dataloader = test_dataloader[g]
        if verbose:
            print(f"Processing data of operator {g + 1} out of {G}")
        for i, batch in enumerate(tqdm(dataloader, disable=not verbose)):
            with torch.no_grad():
                if online_measurements:
                    (
                        x,
                        _,
                    ) = batch  # In this case the dataloader outputs also a class label
                    x = x.to(device)
                    physics_cur = physics[g]
                    if isinstance(physics_cur, torch.nn.DataParallel):
                        physics_cur.module.noise_model.__init__()
                    else:
                        physics_cur.reset()
                    y = physics_cur(x)
                else:
                    x, y = batch
                    if type(x) is list or type(x) is tuple:
                        x = [s.to(device) for s in x]
                    else:
                        x = x.to(device)
                    physics_cur = physics[g]

                    y = y.to(device)

                if plot_metrics:
                    x1, metrics = model(y, physics_cur, x_gt=x, compute_metrics=True)
                else:
                    x1 = model(y, physics[g])

                if hasattr(physics_cur, "A_adjoint"):
                    if isinstance(physics_cur, torch.nn.DataParallel):
                        x_init = physics_cur.module.A_adjoint(y)
                    else:
                        x_init = physics_cur.A_adjoint(y)
                elif hasattr(physics_cur, "A_dagger"):
                    if isinstance(physics_cur, torch.nn.DataParallel):
                        x_init = physics_cur.module.A_dagger(y)
                    else:
                        x_init = physics_cur.A_dagger(y)
                else:
                    x_init = zeros_like(x)

                cur_psnr_init = cal_psnr(x_init, x)
                cur_psnr = cal_psnr(x1, x)
                psnr_init.append(cur_psnr_init)
                psnr_net.append(cur_psnr)

                if plot_images:
                    save_folder_im = (
                        (save_folder / ("G" + str(g))) if G > 1 else save_folder
                    ) / "images"
                    save_folder_im.mkdir(parents=True, exist_ok=True)
                else:
                    save_folder_im = None
                if plot_metrics:
                    save_folder_curve = (
                        (save_folder / ("G" + str(g))) if G > 1 else save_folder
                    ) / "curves"
                    save_folder_curve.mkdir(parents=True, exist_ok=True)

                if plot_images and (step + 1) % img_interval == 0:
                    if g < show_operators:
                        if not plot_only_first_batch or (
                            plot_only_first_batch and i == 0
                        ):
                            if plot_measurements and len(y.shape) == 4:
                                imgs = [y, x_init, x1, x]
                                name_imgs = ["Input", "No learning", "Recons.", "GT"]
                            else:
                                imgs = [x_init, x1, x]
                                name_imgs = ["No learning", "Recons.", "GT"]
                            plot(
                                imgs,
                                titles=name_imgs,
                                save_dir=save_folder_im if plot_images else None,
                                show=plot_images,
                                return_fig=True,
                            )

                if plot_metrics:
                    plot_curves(metrics, save_dir=save_folder_curve, show=True)

    test_psnr = np.mean(psnr_net)
    test_std_psnr = np.std(psnr_net)
    linear_psnr = np.mean(psnr_init)
    linear_std_psnr = np.std(psnr_init)
    if verbose:
        print(
            f"Test PSNR: No learning rec.: {linear_psnr:.2f}+-{linear_std_psnr:.2f} dB | Model: {test_psnr:.2f}+-{test_std_psnr:.2f} dB. "
        )

    return test_psnr, test_std_psnr, linear_psnr, linear_std_psnr


def train(*args, model=None, train_dataloader=None, eval_dataloader=None, **kwargs):
    """
    Alias function for training a model using :class:`deepinv.training_utils.Trainer` class.

    This function creates a Trainer instance and returns the trained model.

    .. warning::

        This function is deprecated and will be removed in future versions. Please use
        :class:`deepinv.training_utils.Trainer` instead.

    :param args: Positional arguments to pass to Trainer constructor.
    :param kwargs: Keyword arguments to pass to Trainer constructor.
    :return: Trained model.
    """
    trainer = Trainer(*args, **kwargs)
    trained_model = trainer.train(
        model=model, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader
    )
    return trained_model


def train_normalizing_flow(
    model,
    dataloader,
    epochs=10,
    learning_rate=1e-3,
    device="cpu",
    jittering=1 / 255.0,
    verbose=False,
):
    r"""
    Trains a normalizing flow.

    Uses the Adam optimizer and the forward Kullback-Leibler (maximum likelihood) loss function given by

    .. math::
        \mathcal{L}(\theta)=\mathrm{KL}(P_X,{\mathcal{T}_\theta}_\#P_Z)=\mathbb{E}_{x\sim P_X}[p_{{\mathcal{T}_\theta}_\#P_Z}(x)]+\mathrm{const},

    where :math:`\mathcal{T}_\theta` is the normalizing flow with parameters :math:`\theta`, latent distribution :math:`P_Z`, data distribution :math:`P_X` and push-forward measure :math:`{\mathcal{T}_\theta}_\#P_Z`.

    :param torch.nn.Module model: Normalizing flow in the same format as in the `FrEIA <https://vislearn.github.io/FrEIA/_build/html/index.html>`_ framework (i.e., the forward method takes the data and the flag rev (default False) where rev=True indicates calling the inverse; the forward method returns the output of the network and the log-determinant of the Jacobian of the flow.
    :param torch.utils.data.DataLoader dataloader: contains training data.
    :param int epochs: number of epochs
    :param float learning_rate: learning rate
    :param str device: used device
    :param float jittering: adds uniform noise of range [-jittering,jittering] to the training data.
        This is a common trick for stabilizing the training of normalizing flows and to avoid overfitting
    :param bool verbose: Whether printing progress.
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    for epoch in range(epochs):
        mean_loss = 0.0
        for i, (x, _) in enumerate(
            progress_bar := tqdm(dataloader, disable=not verbose)
        ):
            x = x.to(device)
            x = x + jittering * (2 * torch.rand_like(x) - 1)
            optimizer.zero_grad()
            invs, jac_inv = model(x)
            loss = torch.mean(
                0.5 * torch.sum(invs.view(invs.shape[0], -1) ** 2, -1)
                - jac_inv.view(invs.shape[0])
            )
            loss.backward()
            optimizer.step()
            mean_loss = mean_loss / (i + 1) * i + loss.item() / (i + 1)
            progress_bar.set_description(
                "Epoch {}, Mean Loss: {:.2f}, Loss {:.2f}".format(
                    epoch + 1, mean_loss, loss.item()
                )
            )
