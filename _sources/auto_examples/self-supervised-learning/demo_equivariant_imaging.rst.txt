
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_equivariant_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:


Self-supervised learning with Equivariant Imaging for MRI.
====================================================================================================

This example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.

The equivariant imaging loss is presented in `"Equivariant Imaging: Learning Beyond the Range Space"
<http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`_.

.. GENERATED FROM PYTHON SOURCE LINES 11-22

.. code-block:: Python


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.optim.prior import PnP
    from deepinv.utils.demo import load_dataset, load_degradation
    from deepinv.training_utils import train, test
    from deepinv.models.utils import get_weights_url








.. GENERATED FROM PYTHON SOURCE LINES 23-26

Setup paths for data loading and results.
---------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 26-39

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    DEG_DIR = BASE_DIR / "degradations"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 40-49

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------
In this example, we use a subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_
as the base image dataset. It consists of 973 knee images of size 320x320.

.. note::

      We reduce to the size to 128x128 for faster training in the demo.


.. GENERATED FROM PYTHON SOURCE LINES 49-63

.. code-block:: Python


    operation = "MRI"
    train_dataset_name = "fastmri_knee_singlecoil"
    img_size = 128

    transform = transforms.Compose([transforms.Resize(img_size)])

    train_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True
    )
    test_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/fastmri_knee_singlecoil.pt
      0%|          | 0.00/399M [00:00<?, ?iB/s]      0%|          | 788k/399M [00:00<00:50, 7.88MiB/s]      2%|▏         | 6.92M/399M [00:00<00:09, 39.3MiB/s]      4%|▎         | 14.7M/399M [00:00<00:06, 56.9MiB/s]      6%|▌         | 22.4M/399M [00:00<00:05, 65.0MiB/s]      8%|▊         | 30.0M/399M [00:00<00:05, 68.7MiB/s]      9%|▉         | 37.7M/399M [00:00<00:05, 71.5MiB/s]     11%|█▏        | 45.4M/399M [00:00<00:04, 73.5MiB/s]     13%|█▎        | 53.2M/399M [00:00<00:04, 75.0MiB/s]     15%|█▌        | 61.1M/399M [00:00<00:04, 76.0MiB/s]     17%|█▋        | 68.9M/399M [00:01<00:04, 76.6MiB/s]     19%|█▉        | 76.6M/399M [00:01<00:04, 76.9MiB/s]     21%|██        | 84.4M/399M [00:01<00:04, 77.2MiB/s]     23%|██▎       | 92.3M/399M [00:01<00:03, 77.6MiB/s]     25%|██▌       | 100M/399M [00:01<00:03, 77.6MiB/s]      27%|██▋       | 108M/399M [00:01<00:03, 77.8MiB/s]     29%|██▉       | 116M/399M [00:01<00:03, 77.8MiB/s]     31%|███       | 123M/399M [00:01<00:03, 77.7MiB/s]     33%|███▎      | 131M/399M [00:01<00:03, 77.9MiB/s]     35%|███▍      | 139M/399M [00:01<00:03, 77.8MiB/s]     37%|███▋      | 147M/399M [00:02<00:03, 77.5MiB/s]     39%|███▉      | 155M/399M [00:02<00:03, 77.6MiB/s]     41%|████      | 162M/399M [00:02<00:03, 77.8MiB/s]     43%|████▎     | 170M/399M [00:02<00:02, 78.1MiB/s]     45%|████▍     | 178M/399M [00:02<00:02, 78.2MiB/s]     47%|████▋     | 186M/399M [00:02<00:02, 78.0MiB/s]     49%|████▊     | 194M/399M [00:02<00:02, 77.5MiB/s]     51%|█████     | 202M/399M [00:02<00:02, 77.7MiB/s]     53%|█████▎    | 209M/399M [00:02<00:02, 78.0MiB/s]     55%|█████▍    | 217M/399M [00:02<00:02, 78.1MiB/s]     56%|█████▋    | 225M/399M [00:03<00:02, 78.2MiB/s]     58%|█████▊    | 233M/399M [00:03<00:02, 78.4MiB/s]     60%|██████    | 241M/399M [00:03<00:02, 78.3MiB/s]     62%|██████▏   | 249M/399M [00:03<00:01, 78.1MiB/s]     64%|██████▍   | 257M/399M [00:03<00:01, 78.1MiB/s]     66%|██████▋   | 264M/399M [00:03<00:01, 77.8MiB/s]     68%|██████▊   | 272M/399M [00:03<00:01, 77.4MiB/s]     70%|███████   | 280M/399M [00:03<00:01, 77.6MiB/s]     72%|███████▏  | 288M/399M [00:03<00:01, 77.8MiB/s]     74%|███████▍  | 296M/399M [00:03<00:01, 77.9MiB/s]     76%|███████▌  | 303M/399M [00:04<00:01, 78.0MiB/s]     78%|███████▊  | 311M/399M [00:04<00:01, 78.1MiB/s]     80%|████████  | 319M/399M [00:04<00:01, 78.5MiB/s]     82%|████████▏ | 327M/399M [00:04<00:00, 78.7MiB/s]     84%|████████▍ | 335M/399M [00:04<00:00, 78.8MiB/s]     86%|████████▌ | 343M/399M [00:04<00:00, 78.7MiB/s]     88%|████████▊ | 351M/399M [00:04<00:00, 78.9MiB/s]     90%|████████▉ | 359M/399M [00:04<00:00, 78.2MiB/s]     92%|█████████▏| 367M/399M [00:04<00:00, 78.4MiB/s]     94%|█████████▍| 374M/399M [00:04<00:00, 78.6MiB/s]     96%|█████████▌| 382M/399M [00:05<00:00, 78.8MiB/s]     98%|█████████▊| 390M/399M [00:05<00:00, 78.5MiB/s]    100%|█████████▉| 398M/399M [00:05<00:00, 78.6MiB/s]    100%|██████████| 399M/399M [00:05<00:00, 76.4MiB/s]




.. GENERATED FROM PYTHON SOURCE LINES 64-68

Generate a dataset of knee images and load it.
----------------------------------------------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 68-98

.. code-block:: Python


    mask = load_degradation("mri_mask_128x128.npy", ORIGINAL_DATA_DIR)

    # defined physics
    physics = dinv.physics.MRI(mask=mask, device=device)

    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        900 if torch.cuda.is_available() else 5
    )  # number of images used for training
    # (the dataset has up to 973 images, however here we use only 900)

    my_dataset_name = "demo_equivariant_imaging"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    mri_mask_128x128.npy degradation downloaded in datasets
    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 140.57it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/19 [00:00<?, ?it/s]    100%|██████████| 19/19 [00:00<00:00, 272.83it/s]
    Dataset has been saved in measurements/fastmri_knee_singlecoil/MRI




.. GENERATED FROM PYTHON SOURCE LINES 99-104

Set up the reconstruction network
---------------------------------------------------------------

As a reconstruction network, we use an unrolled network (half-quadratic splitting)
with a trainable denoising prior based on the DnCNN architecture.

.. GENERATED FROM PYTHON SOURCE LINES 104-149

.. code-block:: Python


    # Select the data fidelity term
    data_fidelity = dinv.optim.L2()
    n_channels = 2  # real + imaginary parts

    # If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each
    # iteration. For fixed trained model prior across iterations, initialize with a single model.
    prior = PnP(
        denoiser=dinv.models.DnCNN(
            in_channels=n_channels,
            out_channels=n_channels,
            pretrained=None,
            train=True,
            depth=7,
        ).to(device)
    )

    # Unrolled optimization algorithm parameters
    max_iter = 3  # number of unfolded layers
    lamb = [1.0] * max_iter  # initialization of the regularization parameter
    stepsize = [1.0] * max_iter  # initialization of the step sizes.
    sigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
    }

    trainable_params = [
        "lambda",
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = dinv.unfolded.unfolded_builder(
        "HQS",
        params_algo=params_algo,
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )









.. GENERATED FROM PYTHON SOURCE LINES 150-163

Set up the training parameters
--------------------------------------------
We choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)
and the equivariant imaging loss (EI).
The EI loss requires a group of transformations to be defined. The forward model `should not be equivariant to
these transformations <https://www.jmlr.org/papers/v24/22-0315.html>`_.
Here we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is
not equivariant to rotations (while it is equivariant to translations).

.. note::

      We use a pretrained model to reduce training time. You can get the same results by training from scratch
      for 150 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 163-188

.. code-block:: Python


    epochs = 1  # choose training epochs
    learning_rate = 5e-4
    batch_size = 16 if torch.cuda.is_available() else 1

    # choose self-supervised training losses
    # generates 4 random rotations per image in the batch
    losses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)

    # start with a pretrained model to reduce training time
    file_name = "new_demo_ei_ckp_150_v3.pth"
    url = get_weights_url(model_name="demo", file_name=file_name)
    ckpt = torch.hub.load_state_dict_from_url(
        url,
        map_location=lambda storage, loc: storage,
        file_name=file_name,
    )
    # load a checkpoint to reduce training time
    model.load_state_dict(ckpt["state_dict"])
    optimizer.load_state_dict(ckpt["optimizer"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/demo/resolve/main/new_demo_ei_ckp_150_v3.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/new_demo_ei_ckp_150_v3.pth
      0%|          | 0.00/2.17M [00:00<?, ?B/s]     22%|██▏       | 480k/2.17M [00:00<00:00, 4.88MB/s]    100%|██████████| 2.17M/2.17M [00:00<00:00, 15.4MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 189-193

Train the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 193-221

.. code-block:: Python



    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False
    )

    train(
        model=model,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        ckp_interval=10,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
      0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/5 [00:02<?, ?it/s, eval_psnr=38.7, loss_mc=5.71e-5, loss_ei=0.000144, total_loss=0.000201, train_psnr=37]    Epoch 1:  20%|██        | 1/5 [00:02<00:08,  2.19s/it, eval_psnr=38.7, loss_mc=5.71e-5, loss_ei=0.000144, total_loss=0.000201, train_psnr=37]    Epoch 1:  20%|██        | 1/5 [00:02<00:08,  2.19s/it, eval_psnr=38.7, loss_mc=5.71e-5, loss_ei=0.000144, total_loss=0.000201, train_psnr=37]    Epoch 1:  20%|██        | 1/5 [00:04<00:08,  2.19s/it, eval_psnr=38.7, loss_mc=4.29e-5, loss_ei=0.000113, total_loss=0.000155, train_psnr=37.8]    Epoch 1:  40%|████      | 2/5 [00:04<00:06,  2.25s/it, eval_psnr=38.7, loss_mc=4.29e-5, loss_ei=0.000113, total_loss=0.000155, train_psnr=37.8]    Epoch 1:  40%|████      | 2/5 [00:04<00:06,  2.25s/it, eval_psnr=38.7, loss_mc=4.29e-5, loss_ei=0.000113, total_loss=0.000155, train_psnr=37.8]    Epoch 1:  40%|████      | 2/5 [00:06<00:06,  2.25s/it, eval_psnr=38.7, loss_mc=3.71e-5, loss_ei=0.000128, total_loss=0.000165, train_psnr=37.6]    Epoch 1:  60%|██████    | 3/5 [00:06<00:04,  2.23s/it, eval_psnr=38.7, loss_mc=3.71e-5, loss_ei=0.000128, total_loss=0.000165, train_psnr=37.6]    Epoch 1:  60%|██████    | 3/5 [00:06<00:04,  2.23s/it, eval_psnr=38.7, loss_mc=3.71e-5, loss_ei=0.000128, total_loss=0.000165, train_psnr=37.6]    Epoch 1:  60%|██████    | 3/5 [00:08<00:04,  2.23s/it, eval_psnr=38.7, loss_mc=4.19e-5, loss_ei=0.000146, total_loss=0.000188, train_psnr=37.1]    Epoch 1:  80%|████████  | 4/5 [00:08<00:02,  2.23s/it, eval_psnr=38.7, loss_mc=4.19e-5, loss_ei=0.000146, total_loss=0.000188, train_psnr=37.1]    Epoch 1:  80%|████████  | 4/5 [00:08<00:02,  2.23s/it, eval_psnr=38.7, loss_mc=4.19e-5, loss_ei=0.000146, total_loss=0.000188, train_psnr=37.1]    Epoch 1:  80%|████████  | 4/5 [00:11<00:02,  2.23s/it, eval_psnr=38.7, loss_mc=4.09e-5, loss_ei=0.000132, total_loss=0.000172, train_psnr=36.9]    Epoch 1: 100%|██████████| 5/5 [00:11<00:00,  2.21s/it, eval_psnr=38.7, loss_mc=4.09e-5, loss_ei=0.000132, total_loss=0.000172, train_psnr=36.9]    Epoch 1: 100%|██████████| 5/5 [00:11<00:00,  2.22s/it, eval_psnr=38.7, loss_mc=4.09e-5, loss_ei=0.000132, total_loss=0.000172, train_psnr=36.9]

    BaseUnfold(
      (fixed_point): FixedPoint(
        (iterator): HQSIteration(
          (f_step): fStepHQS()
          (g_step): gStepHQS()
        )
      )
      (init_params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
      )
      (params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
        )
      )
      (prior): ModuleList(
        (0): PnP(
          (denoiser): DnCNN(
            (in_conv): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv_list): ModuleList(
              (0-4): 5 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (out_conv): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nl_list): ModuleList(
              (0-5): 6 x ReLU()
            )
          )
        )
      )
      (data_fidelity): ModuleList(
        (0): L2()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 222-226

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 226-240

.. code-block:: Python


    plot_images = True
    method = "equivariant_imaging"

    test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation,
        verbose=verbose,
        wandb_vis=wandb_vis,
    )



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :alt: No learning, Recons., GT
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/73 [00:00<?, ?it/s]      1%|▏         | 1/73 [00:00<01:06,  1.08it/s]      3%|▎         | 2/73 [00:01<00:32,  2.20it/s]      4%|▍         | 3/73 [00:01<00:20,  3.39it/s]      5%|▌         | 4/73 [00:01<00:15,  4.54it/s]      7%|▋         | 5/73 [00:01<00:12,  5.58it/s]      8%|▊         | 6/73 [00:01<00:10,  6.49it/s]     10%|▉         | 7/73 [00:01<00:09,  7.24it/s]     11%|█         | 8/73 [00:01<00:08,  7.83it/s]     12%|█▏        | 9/73 [00:01<00:07,  8.28it/s]     14%|█▎        | 10/73 [00:01<00:07,  8.41it/s]     15%|█▌        | 11/73 [00:02<00:07,  8.67it/s]     16%|█▋        | 12/73 [00:02<00:06,  8.90it/s]     18%|█▊        | 13/73 [00:02<00:06,  9.07it/s]     19%|█▉        | 14/73 [00:02<00:06,  9.19it/s]     21%|██        | 15/73 [00:02<00:06,  9.27it/s]     22%|██▏       | 16/73 [00:02<00:06,  9.32it/s]     23%|██▎       | 17/73 [00:02<00:05,  9.37it/s]     25%|██▍       | 18/73 [00:02<00:05,  9.41it/s]     26%|██▌       | 19/73 [00:02<00:05,  9.43it/s]     27%|██▋       | 20/73 [00:02<00:05,  9.17it/s]     29%|██▉       | 21/73 [00:03<00:05,  9.23it/s]     30%|███       | 22/73 [00:03<00:05,  9.31it/s]     32%|███▏      | 23/73 [00:03<00:05,  9.36it/s]     33%|███▎      | 24/73 [00:03<00:05,  9.40it/s]     34%|███▍      | 25/73 [00:03<00:05,  9.43it/s]     36%|███▌      | 26/73 [00:03<00:04,  9.45it/s]     37%|███▋      | 27/73 [00:03<00:04,  9.45it/s]     38%|███▊      | 28/73 [00:03<00:04,  9.43it/s]     40%|███▉      | 29/73 [00:03<00:04,  9.42it/s]     41%|████      | 30/73 [00:04<00:04,  9.15it/s]     42%|████▏     | 31/73 [00:04<00:04,  9.24it/s]     44%|████▍     | 32/73 [00:04<00:04,  9.32it/s]     45%|████▌     | 33/73 [00:04<00:04,  9.37it/s]     47%|████▋     | 34/73 [00:04<00:04,  9.40it/s]     48%|████▊     | 35/73 [00:04<00:04,  9.43it/s]     49%|████▉     | 36/73 [00:04<00:03,  9.44it/s]     51%|█████     | 37/73 [00:04<00:03,  9.46it/s]     52%|█████▏    | 38/73 [00:04<00:03,  9.46it/s]     53%|█████▎    | 39/73 [00:04<00:03,  9.46it/s]     55%|█████▍    | 40/73 [00:05<00:03,  9.20it/s]     56%|█████▌    | 41/73 [00:05<00:03,  9.21it/s]     58%|█████▊    | 42/73 [00:05<00:03,  9.27it/s]     59%|█████▉    | 43/73 [00:05<00:03,  9.33it/s]     60%|██████    | 44/73 [00:05<00:03,  9.37it/s]     62%|██████▏   | 45/73 [00:05<00:02,  9.41it/s]     63%|██████▎   | 46/73 [00:05<00:02,  9.42it/s]     64%|██████▍   | 47/73 [00:05<00:02,  9.43it/s]     66%|██████▌   | 48/73 [00:05<00:02,  9.45it/s]     67%|██████▋   | 49/73 [00:06<00:02,  9.40it/s]     68%|██████▊   | 50/73 [00:06<00:02,  9.05it/s]     70%|██████▉   | 51/73 [00:06<00:02,  9.06it/s]     71%|███████   | 52/73 [00:06<00:02,  9.15it/s]     73%|███████▎  | 53/73 [00:06<00:02,  9.19it/s]     74%|███████▍  | 54/73 [00:06<00:02,  9.20it/s]     75%|███████▌  | 55/73 [00:06<00:01,  9.22it/s]     77%|███████▋  | 56/73 [00:06<00:01,  9.25it/s]     78%|███████▊  | 57/73 [00:06<00:01,  9.31it/s]     79%|███████▉  | 58/73 [00:07<00:01,  9.36it/s]     81%|████████  | 59/73 [00:07<00:01,  9.40it/s]     82%|████████▏ | 60/73 [00:07<00:01,  9.15it/s]     84%|████████▎ | 61/73 [00:07<00:01,  9.21it/s]     85%|████████▍ | 62/73 [00:07<00:01,  9.29it/s]     86%|████████▋ | 63/73 [00:07<00:01,  9.35it/s]     88%|████████▊ | 64/73 [00:07<00:00,  9.40it/s]     89%|████████▉ | 65/73 [00:07<00:00,  9.43it/s]     90%|█████████ | 66/73 [00:07<00:00,  9.45it/s]     92%|█████████▏| 67/73 [00:08<00:00,  9.46it/s]     93%|█████████▎| 68/73 [00:08<00:00,  9.47it/s]     95%|█████████▍| 69/73 [00:08<00:00,  9.48it/s]     96%|█████████▌| 70/73 [00:08<00:00,  9.21it/s]     97%|█████████▋| 71/73 [00:08<00:00,  9.28it/s]     99%|█████████▊| 72/73 [00:08<00:00,  9.33it/s]    100%|██████████| 73/73 [00:08<00:00,  9.38it/s]    100%|██████████| 73/73 [00:08<00:00,  8.44it/s]
    Test PSNR: No learning rec.: 29.39+-3.41 dB | Model: 34.51+-2.95 dB. 

    (34.513077304787835, 2.951144416682947, 29.388851714460817, 3.4114611889544526)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.067 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_equivariant_imaging.ipynb <demo_equivariant_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_equivariant_imaging.py <demo_equivariant_imaging.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
